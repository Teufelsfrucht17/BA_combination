"""Sklearn fallback utilities for environments without PyTorch.

This module mirrors a subset of the PyTorch training pipeline so users can
train and evaluate the model with only ``scikit-learn`` installed.  The helper
is used by ``pipeline.run_train`` whenever importing the PyTorch stack fails
and also powers the standalone ``pipeline.run_train_sklearn`` entry point.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, Iterable, List, Mapping

import numpy as np
import pandas as pd
from loguru import logger
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.model_selection import TimeSeriesSplit
from joblib import dump

from artifact_paths import resolve_backend_paths
from scaler import save_scaler
from metrics import mae_np, mse_np, r2_score_np


@dataclass
class SklearnArtifacts:
    """Artifacts generated by the sklearn fallback training routine."""

    model_path: Path
    predictions_path: Path
    scaler_path: Path
    cv_report_path: Path | None
    metrics: Dict[str, float]
    model_type: str


def _flatten_sequences(X: np.ndarray) -> np.ndarray:
    """Flatten ``(N, T, F)`` sequences to ``(N, T*F)`` feature matrices."""

    if X.ndim != 3:
        raise ValueError(
            "Expected sequences shaped (n_samples, time_steps, n_features)"
        )
    return X.reshape(X.shape[0], -1)


def _time_series_cv(
    X: np.ndarray,
    y: np.ndarray,
    n_splits: int,
    *,
    model_factory: Callable[[], Any],
    label: str,
) -> List[Dict[str, float]]:
    """Run simple time-series cross validation for an arbitrary regressor."""

    splitter = TimeSeriesSplit(n_splits=n_splits)
    results: List[Dict[str, float]] = []

    for fold, (train_idx, val_idx) in enumerate(splitter.split(X), start=1):
        model = model_factory()
        model.fit(X[train_idx], y[train_idx])

        # Train metrics
        y_pred_train = model.predict(X[train_idx])
        train_r2 = r2_score_np(y[train_idx], y_pred_train)

        # Validation metrics
        y_pred = model.predict(X[val_idx])
        metrics = {
            "fold": fold,
            "r2": r2_score_np(y[val_idx], y_pred),
            "mse": mse_np(y[val_idx], y_pred),
            "mae": mae_np(y[val_idx], y_pred),
            "train_r2": train_r2,
        }
        logger.info("Sklearn %s CV fold %d metrics: %s", label, fold, metrics)
        results.append(metrics)

    return results


def _grid_search_alpha(
    X: np.ndarray,
    y: np.ndarray,
    alpha_grid: Iterable[float],
    *,
    n_splits: int,
) -> tuple[float, List[Dict[str, float]]]:
    """Tune Ridge ``alpha`` via time-series CV and return diagnostics."""

    best_alpha = float(next(iter(alpha_grid)))
    best_score = -np.inf
    history: List[Dict[str, float]] = []

    for alpha in alpha_grid:
        splitter = TimeSeriesSplit(n_splits=n_splits)
        scores: List[float] = []
        for train_idx, val_idx in splitter.split(X):
            model = Ridge(alpha=alpha)
            model.fit(X[train_idx], y[train_idx])
            y_pred = model.predict(X[val_idx])
            scores.append(r2_score_np(y[val_idx], y_pred))
        mean_score = float(np.mean(scores)) if scores else float("nan")
        history.append({"alpha": float(alpha), "fold_r2": scores, "mean_r2": mean_score})
        if mean_score > best_score:
            best_alpha = float(alpha)
            best_score = mean_score

    logger.info("Alpha grid search selected alpha=%.5f (mean R2=%.4f)", best_alpha, best_score)
    return best_alpha, history


def _save_cv_report(
    results: List[Dict[str, float]],
    path: Path,
    *,
    alpha_search: List[Dict[str, float]] | None = None,
) -> None:
    """Persist cross-validation metrics (and optional tuning) to ``path``."""

    payload: Dict[str, Any] = {"folds": results}
    if alpha_search:
        payload["alpha_search"] = alpha_search
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2)
    logger.info("Saved sklearn CV report to %s", path)


def train_sklearn_model(
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    *,
    config: Dict[str, Any],
    train_df: pd.DataFrame,
    test_df: pd.DataFrame,
    time_steps: int,
    scaler: Any,
    backend_name: str = "sklearn",
    model_config: Mapping[str, Any] | None = None,
) -> SklearnArtifacts:
    """Train an sklearn regressor and persist the usual pipeline artifacts."""

    sklearn_cfg = config.get("sklearn", {}) if isinstance(config, dict) else {}
    sklearn_cfg = dict(sklearn_cfg) if isinstance(sklearn_cfg, Mapping) else {}
    if model_config:
        sklearn_cfg.update({k: v for k, v in model_config.items() if v is not None})
    sklearn_cfg.pop("models", None)

    model_type = str(sklearn_cfg.get("model_type", sklearn_cfg.get("model", "ridge"))).lower()

    paths_cfg = config.get("paths", {}) if isinstance(config, dict) else {}
    sklearn_paths = resolve_backend_paths(paths_cfg, backend_name)
    alpha = float(sklearn_cfg.get("ridge_alpha", sklearn_cfg.get("alpha", 1.0)))

    X_train_flat = _flatten_sequences(X_train)
    X_test_flat = _flatten_sequences(X_test)

    cv_path: Path | None = None
    tuning_history: List[Dict[str, float]] | None = None
    cv_results: List[Dict[str, float]] = []
    cv_config = config.get("cv", {}) if isinstance(config, dict) else {}
    n_splits = int(cv_config.get("n_splits", 0)) if cv_config else 0
    if model_type in {"ridge", "ridge_regression"}:
        def model_factory() -> Ridge:
            return Ridge(alpha=alpha)

        if n_splits > 1:
            cv_results = _time_series_cv(
                X_train_flat,
                y_train,
                n_splits,
                model_factory=model_factory,
                label="ridge",
            )
            cv_path = sklearn_paths.cv_report

        alpha_grid: List[float] = []
        if isinstance(sklearn_cfg, dict):
            for candidate in sklearn_cfg.get("alpha_grid", []):
                try:
                    value = float(candidate)
                except (TypeError, ValueError):
                    continue
                if value > 0:
                    alpha_grid.append(value)
        if alpha_grid:
            tuning_splits = max(n_splits, 3)
            alpha, tuning_history = _grid_search_alpha(
                X_train_flat,
                y_train,
                alpha_grid,
                n_splits=tuning_splits,
            )

            def model_factory() -> Ridge:
                return Ridge(alpha=alpha)

    elif model_type in {"ols", "linear", "linear_regression"}:
        def model_factory() -> LinearRegression:
            return LinearRegression()

        if n_splits > 1:
            cv_results = _time_series_cv(
                X_train_flat,
                y_train,
                n_splits,
                model_factory=model_factory,
                label="ols",
            )
            cv_path = sklearn_paths.cv_report

    elif model_type in {"random_forest", "rf"}:
        n_estimators = int(sklearn_cfg.get("n_estimators", 200))
        max_depth = sklearn_cfg.get("max_depth")
        max_depth = int(max_depth) if max_depth is not None else None
        min_samples_split = int(sklearn_cfg.get("min_samples_split", 2))
        min_samples_leaf = int(sklearn_cfg.get("min_samples_leaf", 1))
        random_state = int(sklearn_cfg.get("random_state", 42))
        n_jobs = int(sklearn_cfg.get("n_jobs", -1))

        def model_factory() -> RandomForestRegressor:
            params: Dict[str, Any] = {
                "n_estimators": n_estimators,
                "min_samples_split": min_samples_split,
                "min_samples_leaf": min_samples_leaf,
                "random_state": random_state,
                "n_jobs": n_jobs,
            }
            if max_depth is not None:
                params["max_depth"] = max_depth
            return RandomForestRegressor(**params)

        if n_splits > 1:
            cv_results = _time_series_cv(
                X_train_flat,
                y_train,
                n_splits,
                model_factory=model_factory,
                label="random_forest",
            )
            cv_path = sklearn_paths.cv_report

    else:
        raise ValueError(f"Unsupported sklearn model type: {model_type}")

    if (cv_results or tuning_history) and cv_path is None:
        cv_path = sklearn_paths.cv_report
    if cv_path and (cv_results or tuning_history):
        _save_cv_report(cv_results, cv_path, alpha_search=tuning_history)

    model = model_factory()
    model.fit(X_train_flat, y_train)

    # Train metrics
    y_train_pred = model.predict(X_train_flat)
    train_metrics = {
        "train_r2": r2_score_np(y_train, y_train_pred),
        "train_mse": mse_np(y_train, y_train_pred),
        "train_mae": mae_np(y_train, y_train_pred),
    }

    # Test metrics
    y_pred = model.predict(X_test_flat)
    test_metrics = {
        "test_r2": r2_score_np(y_test, y_pred),
        "test_mse": mse_np(y_test, y_pred),
        "test_mae": mae_np(y_test, y_pred),
    }

    metrics = {**train_metrics, **test_metrics}
    logger.info("Sklearn %s metrics (train/test): %s", model_type, metrics)

    # Persist metrics to JSON
    metrics_path = sklearn_paths.metrics
    metrics_path.parent.mkdir(parents=True, exist_ok=True)
    with metrics_path.open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)
    logger.info("Saved sklearn metrics to %s", metrics_path)

    scaler_path = sklearn_paths.scaler
    save_scaler(scaler, scaler_path)
    logger.info("Saved scaler to %s", scaler_path)

    model_path = sklearn_paths.model.with_suffix(".joblib")
    model_path.parent.mkdir(parents=True, exist_ok=True)
    dump(model, model_path)
    logger.info("Saved sklearn model to %s", model_path)

    predictions_path = sklearn_paths.predictions
    predictions_path.parent.mkdir(parents=True, exist_ok=True)
    timestamps = (
        test_df.sort_values(["ric", "ts"])["ts"]
        .iloc[time_steps - 1 : time_steps - 1 + len(y_test)]
        .reset_index(drop=True)
    )
    pd.DataFrame(
        {
            "ts": timestamps,
            "y_true": y_test,
            "y_pred": y_pred,
        }
    ).to_csv(predictions_path, index=False)
    logger.info("Saved sklearn predictions to %s", predictions_path)

    return SklearnArtifacts(
        model_path=model_path,
        predictions_path=predictions_path,
        scaler_path=scaler_path,
        cv_report_path=cv_path,
        metrics=metrics,
        model_type=model_type,
    )
