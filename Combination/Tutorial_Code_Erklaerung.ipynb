{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BA Trading System - Code Erkl√§rung Tutorial\n",
        "\n",
        "Dieses Notebook erkl√§rt den Code des BA Trading Systems Zeile f√ºr Zeile.\n",
        "\n",
        "## Inhaltsverzeichnis\n",
        "\n",
        "1. [Einf√ºhrung](#einf√ºhrung)\n",
        "2. [ConfigManager.py](#configmanager)\n",
        "3. [Datagrabber.py](#datagrabber)\n",
        "4. [Dataprep.py](#dataprep)\n",
        "5. [Models_Wrapper.py](#models-wrapper)\n",
        "6. [ModelComparison.py](#modelcomparison)\n",
        "7. [main.py](#main)\n",
        "8. [Vollst√§ndiges Beispiel](#beispiel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Einf√ºhrung\n",
        "\n",
        "Das BA Trading System ist ein Portfolio-basiertes Machine Learning System f√ºr Aktienprognosen.\n",
        "\n",
        "### Projektstruktur:\n",
        "- **ConfigManager.py**: Verwaltet Konfiguration aus YAML\n",
        "- **Datagrabber.py**: Holt Daten von LSEG/Refinitiv API\n",
        "- **Dataprep.py**: Feature Engineering und Datenaufbereitung\n",
        "- **Models_Wrapper.py**: Wrapper f√ºr ML-Modelle (PyTorch, Sklearn, etc.)\n",
        "- **ModelComparison.py**: Orchestriert Training und Vergleich\n",
        "- **main.py**: Hauptprogramm mit CLI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import notwendiger Bibliotheken\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Stelle sicher, dass wir im richtigen Verzeichnis sind\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "print(\"‚úì Imports erfolgreich\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. ConfigManager.py - Zeile f√ºr Zeile Erkl√§rung\n",
        "\n",
        "Der ConfigManager l√§dt und verwaltet die Konfiguration aus `config.yaml`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZEILE 1-9: Imports\n",
        "# ====================\n",
        "\n",
        "import yaml  # F√ºr YAML-Dateien\n",
        "from pathlib import Path  # F√ºr Pfad-Operationen\n",
        "from typing import Any, Dict  # Type Hints\n",
        "from copy import deepcopy  # F√ºr tiefe Kopien\n",
        "from logger_config import get_logger  # Logging\n",
        "\n",
        "logger = get_logger(__name__)  # Logger f√ºr dieses Modul\n",
        "\n",
        "print(\"‚úì Imports erkl√§rt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZEILE 11-80: DEFAULT_CONFIG\n",
        "# ============================\n",
        "# Dies ist ein Dictionary mit Standard-Werten, die verwendet werden,\n",
        "# wenn die config.yaml fehlt oder unvollst√§ndig ist.\n",
        "\n",
        "DEFAULT_CONFIG = {\n",
        "    \"data\": {\n",
        "        \"portfolios\": {},  # Wird aus config.yaml geladen\n",
        "        \"common_indices\": [],  # Gemeinsame Indizes (z.B. VDAX)\n",
        "        \"fields\": [\"OPEN_PRC\", \"HIGH_1\", \"LOW_1\", \"TRDPRC_1\", \"ACVOL_1\"],\n",
        "        \"periods\": {\n",
        "            \"daily\": {\n",
        "                \"interval\": \"daily\",\n",
        "                \"start\": \"2024-01-01\",\n",
        "                \"end\": \"2025-11-15\"\n",
        "            },\n",
        "            \"intraday\": {\n",
        "                \"interval\": \"30min\",\n",
        "                \"start\": \"2024-01-01\",\n",
        "                \"end\": \"2025-11-15\"\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # ... weitere Defaults\n",
        "}\n",
        "\n",
        "print(\"‚úì DEFAULT_CONFIG erkl√§rt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZEILE 82-100: __init__ Methode\n",
        "# ===============================\n",
        "# Diese Methode wird aufgerufen, wenn ein ConfigManager-Objekt erstellt wird.\n",
        "\n",
        "from ConfigManager import ConfigManager\n",
        "\n",
        "# Beispiel: ConfigManager erstellen\n",
        "config = ConfigManager(\"config.yaml\")\n",
        "\n",
        "print(\"‚úì ConfigManager initialisiert\")\n",
        "print(f\"Config-Pfad: {config.path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZEILE 102-130: _load_and_validate_config()\n",
        "# ===========================================\n",
        "# Diese Methode:\n",
        "# 1. L√§dt die config.yaml\n",
        "# 2. Merged sie mit Defaults\n",
        "# 3. Validiert die Werte\n",
        "\n",
        "# Beispiel: Config-Werte abrufen\n",
        "epochs = config.get(\"models.pytorch_nn.epochs\")\n",
        "print(f\"PyTorch Epochs: {epochs}\")\n",
        "\n",
        "# Punkt-Notation: \"models.pytorch_nn.epochs\" wird zu config[\"models\"][\"pytorch_nn\"][\"epochs\"]\n",
        "features = config.get(\"features.input_features\")\n",
        "print(f\"Features: {features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Datagrabber.py - Zeile f√ºr Zeile Erkl√§rung\n",
        "\n",
        "Der DataGrabber holt Daten von der LSEG/Refinitiv API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZEILE 67-141: fetch_portfolio_data() - DETAILLIERTE ERKL√ÑRUNG\n",
        "# =============================================================\n",
        "\n",
        "print(\"\"\"\n",
        "fetch_portfolio_data(portfolio_name, period_type) macht:\n",
        "\n",
        "1. L√§dt Portfolio-Config aus ConfigManager\n",
        "   - portfolio_name: z.B. \"dax\" oder \"sdax\"\n",
        "   - period_type: \"daily\" oder \"intraday\"\n",
        "\n",
        "2. Konvertiert Datum-Strings zu datetime-Objekten\n",
        "   start = datetime.datetime.strptime(\"2024-01-01\", \"%Y-%m-%d\")\n",
        "   end = datetime.datetime.strptime(\"2025-11-15\", \"%Y-%m-%d\")\n",
        "\n",
        "3. Holt Portfolio-Aktien-Daten\n",
        "   portfolio_df = LS.getHistoryData(\n",
        "       universe=[\"RHMG.DE\", \"ENR1n.DE\", ...],  # Aktien\n",
        "       fields=[\"TRDPRC_1\", \"ACVOL_1\", ...],    # Felder\n",
        "       start=start,\n",
        "       end=end,\n",
        "       interval=\"daily\"\n",
        "   )\n",
        "\n",
        "4. Holt Portfolio-Index (z.B. DAX)\n",
        "   index_df = LS.getHistoryData(\n",
        "       universe=[\".GDAXI\"],\n",
        "       fields=[\"TRDPRC_1\"],\n",
        "       ...\n",
        "   )\n",
        "\n",
        "5. Holt gemeinsame Indizes (z.B. VDAX)\n",
        "   common_df = LS.getHistoryData(\n",
        "       universe=[\".V1XI\"],\n",
        "       ...\n",
        "   )\n",
        "\n",
        "6. Kombiniert alle DataFrames\n",
        "   combined_df = pd.concat([portfolio_df, index_df, common_df], axis=1)\n",
        "\n",
        "7. Speichert als Excel\n",
        "   exceltextwriter(combined_df, \"dax_daily\")\n",
        "\n",
        "8. Gibt DataFrame zur√ºck\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Dataprep.py - Zeile f√ºr Zeile Erkl√§rung\n",
        "\n",
        "Dataprep bereitet Daten f√ºr Machine Learning vor und erstellt Features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZEILE 56-233: create_features() - DETAILLIERTE ERKL√ÑRUNG\n",
        "# ========================================================\n",
        "\n",
        "print(\"\"\"\n",
        "create_features(df, portfolio_name) - Schritt f√ºr Schritt:\n",
        "\n",
        "SCHRITT 1: DataFrame vorbereiten\n",
        "===============================\n",
        "df = df.copy()  # Defensive Kopie (Original bleibt unver√§ndert)\n",
        "\n",
        "# Stelle sicher, dass Index ein DatetimeIndex ist\n",
        "if 'Date' in df.columns:\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df = df.set_index('Date')\n",
        "\n",
        "SCHRITT 2: Fehlende Werte behandeln\n",
        "===================================\n",
        "# Forward Fill (ffill): F√ºlle mit letztem bekannten Wert\n",
        "# Backward Fill (bfill): F√ºlle mit n√§chstem bekannten Wert\n",
        "price_columns = [col for col in df.columns if 'TRDPRC_1' in col]\n",
        "df[price_columns] = df[price_columns].ffill().bfill()\n",
        "\n",
        "SCHRITT 3: Portfolio-Durchschnittspreis berechnen\n",
        "=================================================\n",
        "stock_columns = [col for col in df.columns if '.DE' in col and 'TRDPRC_1' in col]\n",
        "portfolio_prices = df[stock_columns].mean(axis=1)  # Durchschnitt √ºber alle Aktien\n",
        "\n",
        "SCHRITT 4: Momentum-Features\n",
        "===========================\n",
        "for period in [5, 10, 20]:\n",
        "    features[f'momentum_{period}'] = portfolio_prices.pct_change(period)\n",
        "    # pct_change(5) = (price[t] - price[t-5]) / price[t-5]\n",
        "    # Beispiel: momentum_5[100] = (price[100] - price[95]) / price[95]\n",
        "\n",
        "SCHRITT 5: Index-Features (DAX/SDAX)\n",
        "====================================\n",
        "index_columns = [col for col in df.columns if 'GDAXI' in col]\n",
        "index_prices = df[index_columns[0]]\n",
        "features['change_dax'] = index_prices.pct_change()\n",
        "# Prozentuale √Ñnderung des DAX-Index\n",
        "\n",
        "SCHRITT 6: VDAX (Volatilit√§t)\n",
        "=============================\n",
        "vdax_columns = [col for col in df.columns if 'V1XI' in col]\n",
        "features['vdax_absolute'] = df[vdax_columns[0]].abs()\n",
        "# Absoluter Wert des VDAX (Volatilit√§tsindex)\n",
        "\n",
        "SCHRITT 7: Volume-Ratio\n",
        "=======================\n",
        "volume_columns = [col for col in df.columns if 'VOLUME' in col]\n",
        "portfolio_volume = df[volume_columns].mean(axis=1)\n",
        "rolling_mean = portfolio_volume.rolling(20).mean()  # 20-Perioden Durchschnitt\n",
        "features['volume_ratio'] = portfolio_volume / rolling_mean\n",
        "# Verh√§ltnis aktuelles Volume zu Durchschnitt\n",
        "\n",
        "SCHRITT 8: Volatilit√§ts-Features\n",
        "=================================\n",
        "portfolio_returns = np.log(portfolio_prices / portfolio_prices.shift(1))\n",
        "features['rolling_volatility_10'] = portfolio_returns.rolling(10).std()\n",
        "# Standardabweichung der Returns √ºber 10 Perioden\n",
        "\n",
        "SCHRITT 9: Zeit-Features (f√ºr Intraday)\n",
        "========================================\n",
        "if isinstance(df.index, pd.DatetimeIndex):\n",
        "    # Wochentag (0=Montag, 6=Sonntag)\n",
        "    dow = df.index.weekday\n",
        "    features['dow_sin'] = np.sin(2 * np.pi * dow / 7)  # Zyklische Kodierung\n",
        "    features['dow_cos'] = np.cos(2 * np.pi * dow / 7)\n",
        "    \n",
        "    # Stunde (f√ºr Intraday)\n",
        "    hours = df.index.hour\n",
        "    features['hour_sin'] = np.sin(2 * np.pi * hours / 24)\n",
        "    features['hour_cos'] = np.cos(2 * np.pi * hours / 24)\n",
        "\n",
        "SCHRITT 10: Target-Variable\n",
        "===========================\n",
        "features['price_change_next'] = portfolio_returns.shift(-1)\n",
        "# shift(-1) verschiebt um 1 nach oben\n",
        "# price_change_next[t] = return[t+1] (zuk√ºnftiger Return)\n",
        "\n",
        "SCHRITT 11: NaN-Werte entfernen\n",
        "===============================\n",
        "# Entferne erste N Zeilen (wegen Rolling-Windows)\n",
        "features = features.iloc[max(momentum_periods):]\n",
        "# Entferne Zeilen mit NaN im Target\n",
        "features = features.dropna(subset=['price_change_next'])\n",
        "\n",
        "Gibt DataFrame mit Features zur√ºck\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZEILE 292-344: time_series_split() - WICHTIG!\n",
        "# =============================================\n",
        "# Chronologischer Train-Test Split (KEIN Random Shuffle!)\n",
        "\n",
        "print(\"\"\"\n",
        "time_series_split(X, y, test_size=0.2) macht:\n",
        "\n",
        "1. Berechne Split-Index\n",
        "   n_samples = len(X)  # z.B. 1000\n",
        "   split_idx = int(n_samples * (1 - test_size))  # z.B. 800\n",
        "   # Train: 0-800, Test: 800-1000\n",
        "\n",
        "2. Validiere Gr√∂√üen\n",
        "   if n_train < 50:\n",
        "       raise ValueError(\"Trainingsset zu klein\")\n",
        "   if n_test < 10:\n",
        "       raise ValueError(\"Testset zu klein\")\n",
        "\n",
        "3. Split (chronologisch!)\n",
        "   X_train = X.iloc[:split_idx]  # Erste 80%\n",
        "   X_test = X.iloc[split_idx:]   # Letzte 20%\n",
        "   y_train = y.iloc[:split_idx]\n",
        "   y_test = y.iloc[split_idx:]\n",
        "\n",
        "WARUM chronologisch?\n",
        "===================\n",
        "Bei Zeitreihen m√ºssen wir die Zeitordnung erhalten:\n",
        "- Training: Vergangene Daten (z.B. 2024-01-01 bis 2024-12-31)\n",
        "- Test: Zuk√ºnftige Daten (z.B. 2025-01-01 bis 2025-11-15)\n",
        "\n",
        "Random Shuffle w√ºrde Data Leakage verursachen!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Models_Wrapper.py - Zeile f√ºr Zeile Erkl√§rung\n",
        "\n",
        "Models_Wrapper enth√§lt Wrapper-Funktionen f√ºr alle ML-Modelle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZEILE 74-252: train_pytorch_model() - DETAILLIERT\n",
        "# ==================================================\n",
        "\n",
        "print(\"\"\"\n",
        "train_pytorch_model() - Schritt f√ºr Schritt:\n",
        "\n",
        "SCHRITT 1: Seeds setzen (Reproduzierbarkeit)\n",
        "============================================\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "# Gleiche Seeds = gleiche Ergebnisse bei jedem Lauf\n",
        "\n",
        "SCHRITT 2: Device w√§hlen (CPU oder GPU)\n",
        "=======================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# GPU ist schneller, aber nicht immer verf√ºgbar\n",
        "\n",
        "SCHRITT 3: Daten zu Tensoren konvertieren\n",
        "==========================================\n",
        "X_train_t = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
        "# PyTorch braucht Tensoren, nicht DataFrames\n",
        "\n",
        "SCHRITT 4: Validation Split\n",
        "===========================\n",
        "n_train = len(X_train_t)  # z.B. 800\n",
        "val_idx = int(n_train * (1 - validation_split))  # z.B. 640\n",
        "X_train_inner = X_train_t[:val_idx]  # 0-640: Training\n",
        "X_val = X_train_t[val_idx:]          # 640-800: Validation\n",
        "\n",
        "SCHRITT 5: Target standardisieren (optional)\n",
        "============================================\n",
        "y_mean = y_train_inner.mean()\n",
        "y_std = y_train_inner.std()\n",
        "y_train_std = (y_train_inner - y_mean) / y_std\n",
        "# Standardisierung hilft beim Training (bessere Konvergenz)\n",
        "\n",
        "SCHRITT 6: Modell erstellen\n",
        "===========================\n",
        "model = SimpleNet(in_features=n_features, hidden1=128, hidden2=64)\n",
        "model = model.to(device)  # Auf GPU verschieben falls verf√ºgbar\n",
        "\n",
        "SCHRITT 7: Optimizer und Loss\n",
        "=============================\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error\n",
        "\n",
        "SCHRITT 8: Training Loop\n",
        "========================\n",
        "for epoch in range(epochs):  # z.B. 400 Epochen\n",
        "    # Training\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()        # Gradienten zur√ºcksetzen\n",
        "        outputs = model(batch_X)     # Forward Pass\n",
        "        loss = criterion(outputs, batch_y)  # Loss berechnen\n",
        "        loss.backward()              # Backward Pass (Gradienten)\n",
        "        optimizer.step()             # Gewichte updaten\n",
        "    \n",
        "    # Validation\n",
        "    with torch.no_grad():  # Keine Gradienten f√ºr Validation\n",
        "        val_outputs = model(X_val)\n",
        "        val_loss = criterion(val_outputs, y_val_std)\n",
        "    \n",
        "    # Early Stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model.state_dict()  # Speichere beste Gewichte\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            break  # Stoppe Training\n",
        "\n",
        "SCHRITT 9: Bestes Modell laden\n",
        "==============================\n",
        "model.load_state_dict(best_model_state)  # Lade beste Gewichte\n",
        "\n",
        "SCHRITT 10: Predictions und Metriken\n",
        "====================================\n",
        "y_pred = model(X_test)\n",
        "y_pred = y_pred * y_std + y_mean  # Zur√ºck in Original-Skala\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "Gibt (model, metrics) zur√ºck\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. ModelComparison.py - Zeile f√ºr Zeile Erkl√§rung\n",
        "\n",
        "ModelComparison orchestriert den kompletten Workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ZEILE 41-110: run_full_comparison() - HAUPT-WORKFLOW\n",
        "# =====================================================\n",
        "\n",
        "print(\"\"\"\n",
        "run_full_comparison() - Haupt-Workflow:\n",
        "\n",
        "SCHRITT 1: Daten holen\n",
        "======================\n",
        "grabber = DataGrabber(config_path)\n",
        "all_data = grabber.fetch_all_data()\n",
        "# Returns: {\"dax\": {\"daily\": df, \"intraday\": df}, \"sdax\": {...}}\n",
        "\n",
        "SCHRITT 2: F√ºr jedes Portfolio und jede Periode\n",
        "================================================\n",
        "for portfolio_name, portfolio_data in all_data.items():\n",
        "    for period_type, data in portfolio_data.items():\n",
        "        # z.B. portfolio_name=\"dax\", period_type=\"daily\"\n",
        "        \n",
        "        # Datenaufbereitung\n",
        "        prep = DataPrep(config_path)\n",
        "        X, y = prep.prepare_data(data, portfolio_name, period_type)\n",
        "        \n",
        "        # Train-Test Split\n",
        "        X_train, X_test, y_train, y_test = time_series_split(X, y, test_size=0.2)\n",
        "        \n",
        "        # SKALIERUNG (wichtig!)\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_train)  # Fit NUR auf Training!\n",
        "        X_train_scaled = scaler.transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        # Warum nur auf Training fit? -> Verhindert Data Leakage!\n",
        "        \n",
        "        # Modelle trainieren\n",
        "        results = train_all_models(X_train, X_test, y_train, y_test, ...)\n",
        "        \n",
        "        # Ergebnisse speichern\n",
        "        self.results[f\"{portfolio_name}_{period_type}\"] = results\n",
        "\n",
        "SCHRITT 3: Vergleichsbericht erstellen\n",
        "======================================\n",
        "create_comparison_report()\n",
        "# Erstellt Excel mit allen Metriken\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Vollst√§ndiges Beispiel - Wie alles zusammenarbeitet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VOLLST√ÑNDIGES BEISPIEL\n",
        "# ======================\n",
        "\n",
        "print(\"\"\"\n",
        "WORKFLOW - Schritt f√ºr Schritt:\n",
        "\n",
        "1. USER startet Programm:\n",
        "   $ python main.py --mode daily --models pytorch_nn random_forest\n",
        "\n",
        "2. main.py:\n",
        "   - L√§dt config.yaml\n",
        "   - √úberschreibt mit CLI-Argumenten\n",
        "   - Erstellt ModelComparison-Objekt\n",
        "\n",
        "3. ModelComparison.run_full_comparison():\n",
        "   a) DataGrabber.fetch_all_data():\n",
        "      - Holt Daten f√ºr DAX und SDAX\n",
        "      - F√ºr Daily und Intraday\n",
        "      - Speichert als Excel\n",
        "   \n",
        "   b) F√ºr jedes Portfolio/Periode:\n",
        "      - DataPrep.prepare_data():\n",
        "        * Erstellt Features (Momentum, Index, etc.)\n",
        "        * Erstellt X und y\n",
        "      - time_series_split():\n",
        "        * Train: 80%, Test: 20%\n",
        "      - Skalierung:\n",
        "        * StandardScaler auf X_train fitten\n",
        "        * Auf X_train und X_test anwenden\n",
        "      - train_all_models():\n",
        "        * Trainiert PyTorch NN\n",
        "        * Trainiert Random Forest\n",
        "        * Berechnet Metriken\n",
        "   \n",
        "   c) create_comparison_report():\n",
        "      - Sammelt alle Metriken\n",
        "      - Erstellt Excel-Report\n",
        "      - Zeigt beste Modelle\n",
        "\n",
        "4. ERGEBNIS:\n",
        "   - Results/model_comparison.xlsx\n",
        "   - Models/dax_daily/pytorch_nn.pt\n",
        "   - Models/dax_daily/random_forest.pkl\n",
        "   - Logs/main.log\n",
        "\n",
        "DATENFLUSS:\n",
        "===========\n",
        "config.yaml\n",
        "    ‚Üì\n",
        "ConfigManager\n",
        "    ‚Üì\n",
        "DataGrabber ‚Üí DataFrame (Rohdaten)\n",
        "    ‚Üì\n",
        "DataPrep ‚Üí (X, y) (Features + Target)\n",
        "    ‚Üì\n",
        "time_series_split ‚Üí (X_train, X_test, y_train, y_test)\n",
        "    ‚Üì\n",
        "Scaler ‚Üí (X_train_scaled, X_test_scaled)\n",
        "    ‚Üì\n",
        "Models_Wrapper ‚Üí (model, metrics)\n",
        "    ‚Üì\n",
        "ModelComparison ‚Üí Excel-Report\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Wichtige Konzepte\n",
        "\n",
        "### 1. Data Leakage Pr√§vention\n",
        "\n",
        "- **Skalierung**: Scaler wird NUR auf X_train gefittet\n",
        "- **Time Series Split**: Chronologisch, kein Random Shuffle\n",
        "- **Features**: Nur vergangene Informationen\n",
        "\n",
        "### 2. Portfolio-basiertes Training\n",
        "\n",
        "- Ein Modell pro Portfolio (nicht pro Aktie)\n",
        "- Y = Durchschnittliche Return aller Aktien\n",
        "- Robustere Predictions\n",
        "\n",
        "### 3. Feature Engineering\n",
        "\n",
        "- Momentum: Prozentuale √Ñnderung √ºber N Perioden\n",
        "- Index-√Ñnderung: DAX/SDAX Return\n",
        "- Volatilit√§t: Rolling Standard Deviation\n",
        "- Zeit-Features: Zyklische Kodierung (Sinus/Cosinus)\n",
        "\n",
        "### 4. Modell-Vergleich\n",
        "\n",
        "- Alle Modelle werden auf gleichen Daten trainiert\n",
        "- Metriken: R¬≤, MSE, MAE, Directional Accuracy\n",
        "- Excel-Report f√ºr einfachen Vergleich\n",
        "\n",
        "---\n",
        "\n",
        "## Zusammenfassung\n",
        "\n",
        "Dieses Tutorial hat den Code Zeile f√ºr Zeile erkl√§rt:\n",
        "\n",
        "1. **ConfigManager**: L√§dt und validiert Konfiguration\n",
        "2. **DataGrabber**: Holt Daten von API\n",
        "3. **DataPrep**: Feature Engineering und Datenaufbereitung\n",
        "4. **Models_Wrapper**: ML-Modell Training\n",
        "5. **ModelComparison**: Orchestriert Workflow\n",
        "6. **main.py**: Einstiegspunkt\n",
        "\n",
        "**N√§chste Schritte:**\n",
        "- Experimentiere mit verschiedenen Features\n",
        "- Passe Hyperparameter an\n",
        "- F√ºge neue Modelle hinzu\n",
        "\n",
        "Viel Erfolg! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Directional Accuracy - Detaillierte Erkl√§rung\n",
        "\n",
        "**Directional Accuracy** ist eine wichtige Metrik f√ºr Trading-Systeme!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DIRECTIONAL ACCURACY - Was bedeutet das?\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\"\"\n",
        "WAS IST DIRECTIONAL ACCURACY?\n",
        "============================\n",
        "\n",
        "Directional Accuracy misst, wie oft das Modell die RICHTUNG der Preis√§nderung\n",
        "korrekt vorhersagt - also ob der Preis STEIGT (+) oder F√ÑLLT (-).\n",
        "\n",
        "WARUM WICHTIG?\n",
        "=============\n",
        "Bei Aktienprognosen ist es oft wichtiger, die RICHTUNG korrekt vorherzusagen\n",
        "als den exakten Wert. F√ºr Trading-Entscheidungen (Kaufen/Verkaufen) ist die\n",
        "Richtung das Entscheidende!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BEISPIEL-BERECHNUNG\n",
        "\n",
        "# Beispiel: Tats√§chliche Returns\n",
        "y_true = np.array([0.02, -0.01, 0.005, -0.015, 0.01, -0.005, 0.03, -0.02])\n",
        "\n",
        "# Beispiel: Vorhergesagte Returns\n",
        "y_pred = np.array([0.018, -0.008, -0.002, -0.012, 0.009, -0.003, 0.025, -0.018])\n",
        "\n",
        "# Schritt 1: Vorzeichen bestimmen\n",
        "true_signs = np.sign(y_true)  # +1 f√ºr positiv, -1 f√ºr negativ\n",
        "pred_signs = np.sign(y_pred)\n",
        "\n",
        "print(\"Tats√§chliche Returns:\", y_true)\n",
        "print(\"Tats√§chliche Vorzeichen:\", true_signs)\n",
        "print(\"\\nVorhergesagte Returns:\", y_pred)\n",
        "print(\"Vorhergesagte Vorzeichen:\", pred_signs)\n",
        "\n",
        "# Schritt 2: Vergleich\n",
        "correct = (true_signs == pred_signs)\n",
        "\n",
        "print(\"\\nKorrekt?\", correct)\n",
        "\n",
        "# Schritt 3: Anteil berechnen\n",
        "accuracy = np.mean(correct)\n",
        "\n",
        "print(f\"\\n‚úì Directional Accuracy: {accuracy:.1%}\")\n",
        "print(f\"  ({np.sum(correct)} von {len(y_true)} Vorhersagen korrekt)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INTERPRETATION DER WERTE\n",
        "\n",
        "print(\"\"\"\n",
        "WIE INTERPRETIERE ICH DIRECTIONAL ACCURACY?\n",
        "\n",
        "50% (0.50)  ‚Üí Zuf√§llig (M√ºnzwurf)\n",
        "             ‚Üí Modell ist nutzlos\n",
        "\n",
        "55-60%      ‚Üí Leicht besser als Zufall\n",
        "             ‚Üí Modell hat etwas gelernt\n",
        "             ‚Üí F√ºr Finanzdaten: GUT!\n",
        "\n",
        "60-65%      ‚Üí Gut\n",
        "             ‚Üí Modell kann Richtung gut vorhersagen\n",
        "             ‚Üí Potentiell profitabel f√ºr Trading\n",
        "\n",
        ">65%        ‚Üí Sehr gut\n",
        "             ‚Üí Selten bei Finanzdaten!\n",
        "\n",
        "WICHTIG:\n",
        "========\n",
        "Bei Finanzdaten ist bereits 55-60% Directional Accuracy SEHR GUT!\n",
        "Selbst professionelle Trader haben oft nur 52-55% Win-Rate.\n",
        "\n",
        "Deine Werte von 58-62% in der Excel sind also EXCELLENT! üéØ\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PRAKTISCHES BEISPIEL: Trading-Strategie\n",
        "\n",
        "print(\"\"\"\n",
        "WIE KANN ICH DIRECTIONAL ACCURACY NUTZEN?\n",
        "\n",
        "Beispiel-Strategie mit 62% Directional Accuracy:\n",
        "\n",
        "1. Modell sagt \"Preis STEIGT\" voraus (‚Üë)\n",
        "   ‚Üí KAUFE Aktie\n",
        "   ‚Üí 62% Chance auf Gewinn\n",
        "\n",
        "2. Modell sagt \"Preis F√ÑLLT\" voraus (‚Üì)\n",
        "   ‚Üí VERKAUFE Aktie (oder Short)\n",
        "   ‚Üí 62% Chance auf Gewinn\n",
        "\n",
        "Von 100 Trades:\n",
        "- 62 korrekte Vorhersagen ‚Üí Gewinn\n",
        "- 38 falsche Vorhersagen ‚Üí Verlust\n",
        "\n",
        "Wenn durchschnittlicher Gewinn > durchschnittlicher Verlust:\n",
        "‚Üí STRATEGIE IST PROFITABEL! üí∞\n",
        "\n",
        "WARUM WICHTIGER ALS R¬≤?\n",
        "=======================\n",
        "R¬≤ = 0.15 (niedrig) bedeutet:\n",
        "- Exakte Werte schwer vorherzusagen\n",
        "- ABER: Directional Accuracy = 0.62 (gut!) bedeutet:\n",
        "- Richtung kann gut vorhergesagt werden\n",
        "- F√ºr Trading-Entscheidungen ist das oft ausreichend!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CODE-ZEILE F√úR ZEILE ERKL√ÑRUNG\n",
        "\n",
        "print(\"\"\"\n",
        "def directional_accuracy(y_true, y_pred):\n",
        "    \\\"\\\"\\\"Berechnet Trefferrate der Vorzeichen.\\\"\\\"\\\"\n",
        "    \n",
        "    # ZEILE 54-55: Pr√ºfe ob Daten vorhanden\n",
        "    if len(y_true) == 0:\n",
        "        return np.nan  # Keine Daten = NaN\n",
        "    \n",
        "    # ZEILE 56-57: Konvertiere zu Arrays und flatten\n",
        "    y_true = np.asarray(y_true).flatten()\n",
        "    y_pred = np.asarray(y_pred).flatten()\n",
        "    # .flatten() macht sicher, dass es 1D-Array ist\n",
        "    \n",
        "    # ZEILE 58: Berechne Directional Accuracy\n",
        "    return float(np.mean(np.sign(y_true) == np.sign(y_pred)))\n",
        "    \n",
        "    # Schritt f√ºr Schritt:\n",
        "    # 1. np.sign(y_true) ‚Üí [+1, -1, +1, ...]  (Vorzeichen)\n",
        "    # 2. np.sign(y_pred) ‚Üí [+1, -1, -1, ...]  (Vorzeichen)\n",
        "    # 3. == ‚Üí [True, True, False, ...]  (Vergleich)\n",
        "    # 4. np.mean() ‚Üí 0.75  (Anteil True-Werte)\n",
        "    # 5. float() ‚Üí 0.75  (konvertiert zu Float)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VISUELLES BEISPIEL\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Erstelle Beispiel-Datenframe\n",
        "df_example = pd.DataFrame({\n",
        "    'Tats√§chlicher_Return': [0.02, -0.01, 0.005, -0.015, 0.01, -0.005],\n",
        "    'Vorhergesagter_Return': [0.018, -0.008, -0.002, -0.012, 0.009, -0.003],\n",
        "})\n",
        "\n",
        "# Berechne Vorzeichen\n",
        "df_example['True_Sign'] = np.sign(df_example['Tats√§chlicher_Return'])\n",
        "df_example['Pred_Sign'] = np.sign(df_example['Vorhergesagter_Return'])\n",
        "df_example['Korrekt'] = df_example['True_Sign'] == df_example['Pred_Sign']\n",
        "df_example['Richtung_True'] = df_example['True_Sign'].map({1: '‚Üë STEIGT', -1: '‚Üì F√ÑLLT', 0: '‚Üí UNVER√ÑNDERT'})\n",
        "df_example['Richtung_Pred'] = df_example['Pred_Sign'].map({1: '‚Üë STEIGT', -1: '‚Üì F√ÑLLT', 0: '‚Üí UNVER√ÑNDERT'})\n",
        "\n",
        "print(\"BEISPIEL-BERECHNUNG:\")\n",
        "print(\"=\"*80)\n",
        "print(df_example.to_string(index=False))\n",
        "\n",
        "accuracy = df_example['Korrekt'].mean()\n",
        "print(f\"\\n‚úì Directional Accuracy: {accuracy:.1%} ({df_example['Korrekt'].sum()}/{len(df_example)} korrekt)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Was bedeuten die Werte in deiner Excel?\n",
        "\n",
        "**Typische Werte f√ºr Finanzdaten:**\n",
        "- **50%**: Zuf√§llig (wie M√ºnzwurf) - Modell nutzlos\n",
        "- **55-60%**: **Sehr gut** - Modell kann Richtung gut vorhersagen\n",
        "- **>60%**: **Exzellent** - Selten bei Finanzdaten\n",
        "- **>70%**: **Au√üergew√∂hnlich** - Extrem selten\n",
        "\n",
        "**Deine Werte von 58-62% sind sehr gut f√ºr Finanzdaten!**\n",
        "\n",
        "Siehe auch: `DIRECTIONAL_ACCURACY_ERKLAERUNG.md` f√ºr detaillierte Erkl√§rung.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
